{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "text = \"Hey there! ðŸ˜ƒ I can't believe it's already 2024. Did you see John's new blog post? Check it out at https://example.com/blog! Also, email me at john.doe@example.com. He mentioned something about stemming and lemmatizationâ€”interesting stuff. BTW, I'll be attending the AI conference in N.Y.C. next month!! #Excited #AI ðŸ˜Š Let's catch up soon. Cheers, John\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens: ['Hey', 'there', '!', 'ðŸ˜ƒ', 'I', 'ca', \"n't\", 'believe', 'it', \"'s\", 'already', '2024', '.', 'Did', 'you', 'see', 'John', \"'s\", 'new', 'blog', 'post', '?', 'Check', 'it', 'out', 'at', 'https', ':', '//example.com/blog', '!', 'Also', ',', 'email', 'me', 'at', 'john.doe', '@', 'example.com', '.', 'He', 'mentioned', 'something', 'about', 'stemming', 'and', 'lemmatizationâ€”interesting', 'stuff', '.', 'BTW', ',', 'I', \"'ll\", 'be', 'attending', 'the', 'AI', 'conference', 'in', 'N.Y.C', '.', 'next', 'month', '!', '!', '#', 'Excited', '#', 'AI', 'ðŸ˜Š', 'Let', \"'s\", 'catch', 'up', 'soon', '.', 'Cheers', ',', 'John']\n"
     ]
    }
   ],
   "source": [
    "#Preprocessing\n",
    "\n",
    "#1. Tokenization\n",
    "tokens = nltk.word_tokenize(text)\n",
    "print(\"Tokens:\", tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lowercased tokens: ['hey', 'there', '!', 'ðŸ˜ƒ', 'i', 'ca', \"n't\", 'believe', 'it', \"'s\", 'already', '2024', '.', 'did', 'you', 'see', 'john', \"'s\", 'new', 'blog', 'post', '?', 'check', 'it', 'out', 'at', 'https', ':', '//example.com/blog', '!', 'also', ',', 'email', 'me', 'at', 'john.doe', '@', 'example.com', '.', 'he', 'mentioned', 'something', 'about', 'stemming', 'and', 'lemmatizationâ€”interesting', 'stuff', '.', 'btw', ',', 'i', \"'ll\", 'be', 'attending', 'the', 'ai', 'conference', 'in', 'n.y.c', '.', 'next', 'month', '!', '!', '#', 'excited', '#', 'ai', 'ðŸ˜Š', 'let', \"'s\", 'catch', 'up', 'soon', '.', 'cheers', ',', 'john']\n"
     ]
    }
   ],
   "source": [
    "#2. Lowercasing\n",
    "lowercased_tokens = [token.lower() for token in tokens]\n",
    "print(\"Lowercased tokens:\", lowercased_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens without punctuation: ['Hey', 'there', 'ðŸ˜ƒ', 'I', 'ca', \"n't\", 'believe', 'it', \"'s\", 'already', '2024', 'Did', 'you', 'see', 'John', \"'s\", 'new', 'blog', 'post', 'Check', 'it', 'out', 'at', 'https', '//example.com/blog', 'Also', 'email', 'me', 'at', 'john.doe', 'example.com', 'He', 'mentioned', 'something', 'about', 'stemming', 'and', 'lemmatizationâ€”interesting', 'stuff', 'BTW', 'I', \"'ll\", 'be', 'attending', 'the', 'AI', 'conference', 'in', 'N.Y.C', 'next', 'month', 'Excited', 'AI', 'ðŸ˜Š', 'Let', \"'s\", 'catch', 'up', 'soon', 'Cheers', 'John']\n"
     ]
    }
   ],
   "source": [
    "#3. Remove Punctuation\n",
    "import string\n",
    "filtered_tokens = [token for token in tokens if token not in string.punctuation]\n",
    "\n",
    "print(\"Tokens without punctuation:\", filtered_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens without stopwords: ['Hey', '!', 'ðŸ˜ƒ', 'ca', \"n't\", 'believe', \"'s\", 'already', '2024', '.', 'see', 'John', \"'s\", 'new', 'blog', 'post', '?', 'Check', 'https', ':', '//example.com/blog', '!', 'Also', ',', 'email', 'john.doe', '@', 'example.com', '.', 'mentioned', 'something', 'stemming', 'lemmatizationâ€”interesting', 'stuff', '.', 'BTW', ',', \"'ll\", 'attending', 'AI', 'conference', 'N.Y.C', '.', 'next', 'month', '!', '!', '#', 'Excited', '#', 'AI', 'ðŸ˜Š', 'Let', \"'s\", 'catch', 'soon', '.', 'Cheers', ',', 'John']\n"
     ]
    }
   ],
   "source": [
    "#4. Removing stopwords\n",
    "#nltk.download('stopwords')\n",
    "# get list of stopwords in English\n",
    "stopwords = nltk.corpus.stopwords.words(\"english\")\n",
    "\n",
    "# remove stopwords\n",
    "filtered_tokens2 = [token for token in tokens if token.lower() not in stopwords]\n",
    "\n",
    "print(\"Tokens without stopwords:\", filtered_tokens2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned text: Hey there! ðŸ˜ƒ I can't believe it's already 2024. Did you see John's new blog post? Check it out at https://example.com/blog! Also, email me at john.doe@example.com. He mentioned something about stemming and lemmatizationâ€”interesting stuff. BTW, I'll be attending the AI conference in N.Y.C. next month!! #Excited #AI ðŸ˜Š Let's catch up soon. Cheers, John\n"
     ]
    }
   ],
   "source": [
    "#5. Removing whitespaces\n",
    "# remove leading and trailing white space\n",
    "text1 = text.strip()\n",
    "\n",
    "# replace multiple consecutive white space characters with a single space\n",
    "text1 = \" \".join(text.split())\n",
    "\n",
    "print(\"Cleaned text:\", text1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text without URLs: Hey there! ðŸ˜ƒ I can't believe it's already 2024. Did you see John's new blog post? Check it out at ! Also, email me at john.doe@example.com. He mentioned something about stemming and lemmatizationâ€”interesting stuff. BTW, I'll be attending the AI conference in N.Y.C. next month!! #Excited #AI ðŸ˜Š Let's catch up soon. Cheers, John\n"
     ]
    }
   ],
   "source": [
    "#6. Removing URLs\n",
    "import re\n",
    "\n",
    "# define a regular expression pattern to match URLs\n",
    "pattern = r\"(http|ftp|https)://([\\w_-]+(?:(?:\\.[\\w_-]+)+))([\\w.,@?^=%&:/~+#-]*[\\w@?^=%&/~+#-])?\"\n",
    "\n",
    "# replace URLs with an empty string\n",
    "cleaned_text = re.sub(pattern, \"\", text)\n",
    "\n",
    "print(\"Text without URLs:\", cleaned_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text without HTML code: Hey there! ðŸ˜ƒ I can't believe it's already 2024. Did you see John's new blog post? Check it out at https://example.com/blog! Also, email me at john.doe@example.com. He mentioned something about stemming and lemmatizationâ€”interesting stuff. BTW, I'll be attending the AI conference in N.Y.C. next month!! #Excited #AI ðŸ˜Š Let's catch up soon. Cheers, John\n"
     ]
    }
   ],
   "source": [
    "#7. Removing HTML tags\n",
    "pattern2 = r\"<[^>]+>\"\n",
    "\n",
    "# replace HTML tags with an empty string\n",
    "cleaned_text2 = re.sub(pattern2, \"\", text)\n",
    "\n",
    "print(\"Text without HTML code:\", cleaned_text2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens without frequent words: ['Hey', 'there', '!', 'ðŸ˜ƒ', 'I', 'ca', \"n't\", 'believe', 'it', \"'s\", 'already', '2024', '.', 'Did', 'you', 'see', 'John', \"'s\", 'new', 'blog', 'post', '?', 'Check', 'it', 'out', 'at', 'https', ':', '//example.com/blog', '!', 'Also', ',', 'email', 'me', 'at', 'john.doe', '@', 'example.com', '.', 'He', 'mentioned', 'something', 'about', 'stemming', 'and', 'lemmatizationâ€”interesting', 'stuff', '.', 'BTW', ',', 'I', \"'ll\", 'be', 'attending', 'the', 'AI', 'conference', 'in', 'N.Y.C', '.', 'next', 'month', '!', '!', '#', 'Excited', '#', 'AI', 'ðŸ˜Š', 'Let', \"'s\", 'catch', 'up', 'soon', '.', 'Cheers', ',', 'John']\n"
     ]
    }
   ],
   "source": [
    "#8. Removing Frequent words\n",
    "# calculate the frequency of each word\n",
    "fdist = nltk.FreqDist(tokens)\n",
    "\n",
    "# remove the most common words (e.g., the top 10% of words by frequency)\n",
    "filtered_tokens3 = [token for token in tokens if fdist[token] < fdist.N() * 0.1]\n",
    "\n",
    "print(\"Tokens without frequent words:\", filtered_tokens3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
